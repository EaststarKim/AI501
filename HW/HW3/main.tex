\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[titletoc,title]{appendix}
\usepackage{fancyhdr}
\usepackage[T1]{fontenc}
\usepackage{bm}
\usepackage{amsmath,amsthm,amsfonts,amssymb,mathtools}
\newcommand{\bmat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\norm}[2][]{\lVert#2\rVert_{#1}}
\usepackage{graphicx,float}
\usepackage{changepage}

\pagestyle{fancy} \fancyhf{}
\lhead{Homework 3} \rhead{AI501}
\rfoot{\thepage}
\title{Homework 3}
\author{20213073 Donggyu Kim}
\date{April 30, 2021}

\begin{document}
\maketitle
\thispagestyle{fancy}

\begin{enumerate}
\item
\begin{enumerate}
    \item True
    \item True
    \item True
    \item True
    \item True
\end{enumerate}
\item
For each following function, $\operatorname{dom}(f)=\mathbb{R}^{n}$ is a convex set. Therefore, we only need to show the Jensen inequality holds to prove that each function is convex.
\begin{enumerate}
    \item
    $\forall \bm x,\bm y\in\mathbb{R}^{n},\ \forall t\in [0,1],$
    \begin{align*}
    f(t\bm x+(1-t)\bm y) &= \max(tx_{1}+(1-t)y_{1},\dots,tx_{n}+(1-t)y_{n}) \\
    &\le t\max(x_{1},\dots,x_{n})+(1-t)\max(y_{1},\dots,y_{n}) \\
    &= tf(\bm x)+(1-t)f(\bm y)
    \end{align*}
    \item
    $\forall \bm x,\bm y\in\mathbb{R}^{n},\ \forall t\in [0,1],$
    \begin{align*}
    f(t\bm x+(1-t)\bm y) &= \norm[p]{t\bm x+(1-t)\bm y} \\
    &\le \norm[p]{t\bm x}+\norm[p]{(1-t)\bm y}
    && (\because \text{Subadditivity}) \\
    &= t\norm[p]{\bm x}+(1-t)\norm[p]{\bm y}
    && (\because \text{Homogeneity}) \\
    &= tf(\bm x)+(1-t)f(\bm y)
    \end{align*}
    \item
    $\forall \bm x,\bm y\in\mathbb{R}^{n},\ \forall t\in [0,1],$
    \begin{align*}
    f(t\bm x+(1-t)\bm y)
    &= \log\left(\sum_{i=1}^{n}e^{tx_{i}+(1-t)y_{i}}\right) \\
    &= \log\left(\sum_{i=1}^{n}e^{tx_{i}}e^{(1-t)y_{i}}\right) \\
    &\le \log\left[\left\{\sum_{i=1}^{n}\left(e^{tx_{i}}\right)^{\frac{1}{t}}\right\}^{t}\left\{\sum_{i=1}^{n}\left(e^{(1-t)y_{i}}\right)^{\frac{1}{1-t}}\right\}^{1-t}\right] && (\because \text{H\"older's inequality}) \\
    &= t\log\left(\sum_{i=1}^{n}e^{x_{i}}\right)+(1-t)\log\left(\sum_{i=1}^{n}e^{y_{i}}\right) \\
    &= tf(\bm x)+(1-t)f(\bm y)
    \end{align*}
\end{enumerate}
\item
\begin{enumerate}
    \item
    \begin{proof}$ $\\
    $\Rightarrow)\ \forall x,y\in\operatorname{dom}(f),\ \forall t\in(0,1],$
    \begin{align*}
    f(y) &= \frac{tf(y)+(1-t)f(x)-(1-t)f(x)}{t} \\
    &\ge \frac{f(ty+(1-t)x)-(1-t)f(x)}{t} && (\because \text{Convexity}) \\
    &= f(x)+\frac{f(x+t(y-x))-f(x)}{t} \\
    \end{align*}
    Since $f$ is differentiable, we get
    \begin{align*}
    f(y) &= \lim_{t\to 0+}f(y) \\
    &\ge \lim_{t\to 0+}\left(f(x)+\frac{f(x+t(y-x))-f(x)}{t}\right) \\
    &= f(x)+f'(x)(y-x)
    \end{align*}
    $\Leftarrow)$ Let $z=tx+(1-t)y$ for any $x,y\in\operatorname{dom}(f),\ t\in[0,1]$. By the inequality,
    \begin{align*}
    tf(x)+(1-t)f(y) &\ge t(f(z)+f'(z)(x-z))+(1-t)(f(z)+f'(z)(y-z)) \\
    &= f(z)+f'(z)(tx+(1-t)y-z) \\
    &= f(z) = f(tx+(1-t)y)
    \end{align*}
    which means that $f$ is convex.
    \end{proof}
    \item
    \begin{proof}$ $\\
    As we already proved (a), let's prove that $\forall x,y\in\operatorname{dom}(f),\ f(y)\ge f(x)+f'(x)(y-x)\Longleftrightarrow \forall z\in\operatorname{dom}(f),\ f''(z)\ge 0$ to solve this problem.
    
    $\Rightarrow)\ \forall x,y\in\operatorname{dom}(f),$
    \begin{align*}
    f'(x)(y-x) &= f(x)+f'(x)(y-x)-f(x) \\
    &\le f(y)-f(x) \\
    &\le f(y)-(f(y)+f'(y)(x-y)) \\
    &= f'(y)(y-x)
    \end{align*}
    Thus, we have
    \begin{align*}
    f''(x) &= \lim_{y\to x}\frac{f'(y)-f'(x)}{y-x} \\
    &= \lim_{y\to x}\frac{(f'(y)-f'(x))(y-x)}{(y-x)^2} \\
    &\ge 0
    \end{align*}
    $\Leftarrow)$ By the mean value version of the Taylor's theorem,
    \[\forall x,y\in\operatorname{dom}(f),\ \exists z\in[x,y] \text{\quad s.t.\quad} f(y)=f(x)+f'(x)(y-x)+\frac{1}{2}f''(z)(y-x)^{2}\]
    Since $f''(z)\ge 0$, we can get
    \[f(y) \ge f(x)+f'(x)(y-x)\]
    Thus, $f$ is convex iff $\forall z\in\operatorname{dom}(f),\ f''(z)\ge 0$.
    \end{proof}
\end{enumerate}
\item
    The Lagrangian of the primal problem is as follows.
    \[\mathcal{L}(\bm w,b,\bm\lambda)=\frac{1}{2}\bm w^{\top}\bm w+\sum_{i=1}^{N}\lambda_{i}\{1-y_{i}(\bm w^{\top}\bm x_{i}+b)\}\]
    Then we can rewrite the primal problem with the Lagrangian.
    \begin{align*}
    \underset{\bm w,b}{\operatorname{minimize}}\quad & \sup_{\bm\lambda}\mathcal{L}(\bm w,b,\bm\lambda) \\
    \text{s.t.}\quad & \bm\lambda \ge \bm 0
    \end{align*}
    We can use $\frac{\partial\mathcal{L}}{\partial\bm w}=0$ and $\frac{\partial\mathcal{L}}{\partial b}=0$ to derive $\inf_{\bm w,b}\mathcal{L}(\bm w,b,\bm\lambda)$.
    \begin{align*}
    \frac{\partial\mathcal{L}}{\partial\bm w}
    &= \bm w^{\top}-\sum_{i=1}^{N}\lambda_{i}y_{i}\bm x_{i}^{\top}
    \Rightarrow \bm w=\sum_{i=1}^{N}\lambda_{i}y_{i}\bm x_{i} \\
    \frac{\partial\mathcal{L}}{\partial b}
    &= -\sum_{i=1}^{N}\lambda_{i}y_{i}=0
    \Rightarrow \sum_{i=1}^{N}\lambda_{i}y_{i}=0
    \end{align*}
    By using these conditions, we have
    \begin{align*}
    \inf_{\bm w,b}\mathcal{L}(\bm w,b,\bm\lambda)
    &= \frac{1}{2}\bm w^{\top}\bm w+\sum_{i=1}^{N}\lambda_{i}-\bm w^{\top}\sum_{i=1}^{N}\lambda_{i}y_{i}\bm x_{i}-\left(\sum_{i=1}^{n}\lambda_{i}y_{i}\right)\!b \\
    &= \frac{1}{2}\bm w^{\top}\bm w+\sum_{i=1}^{N}\lambda_{i}-\bm w^{\top}\bm w \\
    &= -\frac{1}{2}\left(\sum_{i=1}^{N}\lambda_{i}y_{i}\bm x_{i}\right)^{\!\top}\!\left(\sum_{i=1}^{N}\lambda_{i}y_{i}\bm x_{i}\right)
    +\sum_{i=1}^{N}\lambda_{i} \\
    &= -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_{i}\lambda_{j}y_{i}y_{j}\bm x_{i}^{\top}\bm x_{j}+\sum_{i=1}^{N}\lambda_{i}
    \end{align*}
    Thus, the Lagrangian dual problem of the primal is as below.
    \begin{align*}
    \underset{\bm\lambda}{\operatorname{maximize}}\quad & \sum_{i=1}^{N}\lambda_{i}-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_{i}\lambda_{j}y_{i}y_{j}\bm x_{i}^{\top}\bm x_{j} \\
    \text{s.t.}\quad & \bm\lambda \ge \bm 0, \\
    & \sum_{i=1}^{N}\lambda_{i}y_{i}=0
    \end{align*}
    There exist $\bm w$ and $b$ that satisfy $1-y_{i}(\bm w^{\top}\bm x_{i}+b)\le 0$ for $i=1,\dots,N$. Thus, $(2\bm w, 2b)$ is strictly feasible; $1-y_{i}((2\bm w)^{\top}\bm x_{i}+(2b))\le -1<0$. Since the convex problem satisfies the Slater's condition, the strong duality holds.
\end{enumerate}
\end{document}
